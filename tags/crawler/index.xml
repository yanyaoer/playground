<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>crawler on yányào.com</title><link>https://y%C3%A1ny%C3%A0o.com/tags/crawler/</link><description>Recent content in crawler on yányào.com</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 17 Oct 2015 12:06:21 +0000</lastBuildDate><atom:link href="https://y%C3%A1ny%C3%A0o.com/tags/crawler/index.xml" rel="self" type="application/rss+xml"/><item><title>simple crawler</title><link>https://y%C3%A1ny%C3%A0o.com/posts/simple-nodejs-crawler/</link><pubDate>Sat, 17 Oct 2015 12:06:21 +0000</pubDate><guid>https://y%C3%A1ny%C3%A0o.com/posts/simple-nodejs-crawler/</guid><description>半夜看了本小说觉得翻页太累
首先要解决的问题是找一个质量还过得去的小说站 然后写个脚本去把它爬下来章节合并到一起
import fs from &amp;#34;fs&amp;#34;; import { argv } from &amp;#34;process&amp;#34;; import request from &amp;#34;request&amp;#34;; import cheerio from &amp;#34;cheerio&amp;#34;; import iconv from &amp;#34;iconv-lite&amp;#34;; import sanitize from &amp;#39;sanitize-html&amp;#39;; class main { constructor() { Object.assign(this, { path: &amp;#39;./chapter.json&amp;#39;, html: &amp;#39;./reader.html&amp;#39;, url: { list: &amp;#39;http://www.piaotian.net/html/6/6658/&amp;#39; }, store: [] }) } fetch(url, callback) { request.get(url, {encoding: null}, (error, response, body)=&amp;gt; { if (!error &amp;amp;&amp;amp; response.statusCode == 200) { let $ = cheerio.load(iconv.decode(body, &amp;#39;GBK&amp;#39;)); callback($, body); } else { console.</description></item></channel></rss>